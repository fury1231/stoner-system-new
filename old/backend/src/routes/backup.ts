import { Router, Request, Response } from 'express'
import { db } from '../db.js'
import { authenticate } from '../middleware/auth.js'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'
import jwt from 'jsonwebtoken'
import multer from 'multer'

// ğŸ”§ çµ±ä¸€çš„è³‡æ–™åº«è·¯å¾‘å‡½æ•¸ï¼ˆèˆ‡ db.ts ä¿æŒä¸€è‡´ï¼‰
function getDatabasePath(): string {
  const dbDir = process.env.NODE_ENV === 'production' 
    ? path.join(process.cwd(), 'data') 
    : process.cwd()
  return path.join(dbDir, 'database.sqlite')
}

const router: Router = Router()
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

// è¨­ç½® multer ç”¨æ–¼æª”æ¡ˆä¸Šå‚³
const upload = multer({
  dest: path.join(__dirname, '../../uploads/'),
  limits: {
    fileSize: 100 * 1024 * 1024, // 100MB é™åˆ¶
  },
  fileFilter: (req, file, cb) => {
    // åªå…è¨± .db å’Œ .sql æª”æ¡ˆ
    const allowedExtensions = ['.db', '.sql', '.sqlite', '.sqlite3']
    const fileExtension = path.extname(file.originalname).toLowerCase()
    
    if (allowedExtensions.includes(fileExtension)) {
      cb(null, true)
    } else {
      cb(new Error('åªå…è¨±ä¸Šå‚³ .db, .sql, .sqlite, .sqlite3 æª”æ¡ˆ'))
    }
  }
})

// æ¬Šé™æª¢æŸ¥ä¸­é–“ä»¶ - åªæœ‰ç®¡ç†å“¡èƒ½å‚™ä»½
const requireAdmin = async (req: any, res: Response, next: any) => {
  try {
    const user = await db.getUserByUsername(req.user.username)
    if (!user || user.role !== 'admin') {
      return res.status(403).json({ message: 'æ¬Šé™ä¸è¶³ï¼šåƒ…ç®¡ç†å“¡å¯åŸ·è¡Œå‚™ä»½æ“ä½œ' })
    }
    next()
  } catch (error) {
    console.error('Permission check error:', error)
    res.status(500).json({ message: 'ä¼ºæœå™¨éŒ¯èª¤' })
  }
}

// å‚™ä»½è³‡æ–™åº«
router.post('/database', authenticate, requireAdmin, async (req: Request, res: Response): Promise<void> => {
  try {
    console.log('=== Database Backup Started ===')
    const backupResult = await createDatabaseBackup(req as any)
    
    // è¨˜éŒ„å¯©è¨ˆæ—¥èªŒ
    const backupUser = await db.getUserByUsername((req as any).user.username)
    await db.createAuditLog({
      user_id: backupUser?.id || 0,
      username: (req as any).user.username,
      action: 'create',
      resource_type: 'system',
      resource_id: 'database_backup',
      details: `è³‡æ–™åº«å‚™ä»½æˆåŠŸ: ${backupResult.filename}, å¤§å°: ${backupResult.size} bytes`,
      ip_address: req.ip,
      user_agent: req.get('User-Agent')
    })
    
    console.log('=== Database Backup Completed ===')
    res.json({
      success: true,
      message: 'è³‡æ–™åº«å‚™ä»½æˆåŠŸ',
      backup: backupResult
    })
  } catch (error) {
    console.error('Database backup error:', error)
    
    // è¨˜éŒ„éŒ¯èª¤å¯©è¨ˆæ—¥èªŒ
    try {
      const errorUser = await db.getUserByUsername((req as any).user.username)
      await db.createAuditLog({
        user_id: errorUser?.id || 0,
        username: (req as any).user.username,
        action: 'create',
        resource_type: 'system',
        resource_id: 'database_backup',
        details: `è³‡æ–™åº«å‚™ä»½å¤±æ•—: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
    } catch (auditError) {
      console.error('Failed to log backup error:', auditError)
    }
    
    res.status(500).json({ 
      success: false,
      message: 'è³‡æ–™åº«å‚™ä»½å¤±æ•—',
      error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'
    })
  }
})

// ä¸‹è¼‰å‚™ä»½æª”æ¡ˆï¼ˆæ”¯æ´ token æŸ¥è©¢åƒæ•¸ï¼‰
router.get('/download/:filename', async (req: Request, res: Response): Promise<void> => {
  try {
    console.log('=== Backup Download Request ===')
    console.log('Filename:', req.params.filename)
    console.log('Query token:', req.query.token ? 'Present' : 'Missing')
    console.log('Auth header:', req.headers.authorization ? 'Present' : 'Missing')
    
    // å¾æŸ¥è©¢åƒæ•¸æˆ–æ¨™é ­ç²å– token
    const token = req.query.token as string || req.headers.authorization?.replace('Bearer ', '')
    
    if (!token) {
      console.log('âŒ No token provided')
      res.status(401).json({ message: 'éœ€è¦èªè­‰ token' })
      return
    }
    
    // ç²å– JWT_SECRET
    const jwtSecret = process.env.JWT_SECRET
    console.log('JWT_SECRET exists:', !!jwtSecret)
    
    if (!jwtSecret || jwtSecret === 'default-secret') {
      console.log('âŒ Invalid JWT_SECRET configuration')
      res.status(500).json({ message: 'ç³»çµ±é…ç½®éŒ¯èª¤' })
      return
    }
    
    // é©—è­‰ token å’Œæ¬Šé™
    let decoded: any
    try {
      decoded = jwt.verify(token, jwtSecret) as any
      console.log('âœ… Token verified, username:', decoded.username)
    } catch (authError) {
      console.log('âŒ Token verification failed:', authError)
      res.status(401).json({ message: 'ç„¡æ•ˆçš„èªè­‰ token' })
      return
    }
    
    // æª¢æŸ¥ç”¨æˆ¶æ¬Šé™
    const user = await db.getUserByUsername(decoded.username)
    if (!user) {
      console.log('âŒ User not found:', decoded.username)
      res.status(401).json({ message: 'ç”¨æˆ¶ä¸å­˜åœ¨' })
      return
    }
    
    if (user.role !== 'admin') {
      console.log('âŒ Insufficient permissions, user role:', user.role)
      res.status(403).json({ message: 'æ¬Šé™ä¸è¶³ï¼šåƒ…ç®¡ç†å“¡å¯ä¸‹è¼‰å‚™ä»½' })
      return
    }
    
    console.log('âœ… User permissions verified')
    
    const filename = req.params.filename
    const backupDir = path.join(__dirname, '../../backups')
    const filePath = path.join(backupDir, filename)
    
    console.log('Backup directory:', backupDir)
    console.log('File path:', filePath)
    
    // å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æª”æ¡ˆè·¯å¾‘åœ¨å‚™ä»½ç›®éŒ„å…§
    const normalizedBackupDir = path.resolve(backupDir)
    const normalizedFilePath = path.resolve(filePath)
    
    if (!normalizedFilePath.startsWith(normalizedBackupDir)) {
      console.log('âŒ Invalid file path, potential directory traversal')
      res.status(400).json({ message: 'ç„¡æ•ˆçš„æª”æ¡ˆè·¯å¾‘' })
      return
    }
    
    // æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if (!fs.existsSync(filePath)) {
      console.log('âŒ File does not exist:', filePath)
      // åˆ—å‡ºå¯ç”¨æª”æ¡ˆå¹«åŠ©é™¤éŒ¯
      try {
        const availableFiles = fs.readdirSync(backupDir)
        console.log('Available files:', availableFiles)
      } catch (dirError) {
        console.log('Cannot list directory:', dirError)
      }
      res.status(404).json({ message: 'å‚™ä»½æª”æ¡ˆä¸å­˜åœ¨' })
      return
    }
    
    console.log('âœ… File exists, preparing download')
    
    // è¨˜éŒ„ä¸‹è¼‰å¯©è¨ˆæ—¥èªŒ
    try {
      await db.createAuditLog({
        user_id: user.id,
        username: decoded.username,
        action: 'view',
        resource_type: 'system',
        resource_id: 'database_backup',
        details: `ä¸‹è¼‰å‚™ä»½æª”æ¡ˆ: ${filename}`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
      console.log('âœ… Audit log created')
    } catch (auditError) {
      console.log('âš ï¸  Failed to create audit log:', auditError)
      // ä¸å½±éŸ¿ä¸‹è¼‰ï¼Œç¹¼çºŒåŸ·è¡Œ
    }
    
    // è¨­å®šä¸‹è¼‰æ¨™é ­
    res.setHeader('Content-Disposition', `attachment; filename="${filename}"`)
    res.setHeader('Content-Type', 'application/octet-stream')
    
    console.log('âœ… Sending file...')
    // å‚³é€æª”æ¡ˆ
    res.sendFile(filePath, (err) => {
      if (err) {
        console.log('âŒ Error sending file:', err)
      } else {
        console.log('âœ… File sent successfully')
      }
    })
  } catch (error) {
    console.error('âŒ Download backup error:', error)
    res.status(500).json({ message: 'ä¸‹è¼‰å‚™ä»½æª”æ¡ˆå¤±æ•—' })
  }
})

// åˆ—å‡ºå¯ç”¨çš„å‚™ä»½æª”æ¡ˆ
router.get('/list', authenticate, requireAdmin, async (req: Request, res: Response): Promise<void> => {
  try {
    const backupDir = path.join(__dirname, '../../backups')
    
    // ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
    if (!fs.existsSync(backupDir)) {
      fs.mkdirSync(backupDir, { recursive: true })
    }
    
    const files = fs.readdirSync(backupDir)
      .filter(file => file.endsWith('.sql') || file.endsWith('.db'))
      .map(file => {
        const filePath = path.join(backupDir, file)
        const stats = fs.statSync(filePath)
        return {
          filename: file,
          size: stats.size,
          created: stats.birthtime,
          modified: stats.mtime
        }
      })
      .sort((a, b) => b.created.getTime() - a.created.getTime())
    
    res.json({
      success: true,
      backups: files
    })
  } catch (error) {
    console.error('List backups error:', error)
    res.status(500).json({ message: 'ç²å–å‚™ä»½åˆ—è¡¨å¤±æ•—' })
  }
})

// åŒ¯å…¥è³‡æ–™åº«
router.post('/import', authenticate, requireAdmin, upload.single('database'), async (req: Request, res: Response): Promise<void> => {
  try {
    console.log('=== Database Import Started ===')
    
    if (!req.file) {
      res.status(400).json({ 
        success: false, 
        message: 'æœªæä¾›è³‡æ–™åº«æª”æ¡ˆ' 
      })
      return
    }
    
    const uploadedFile = req.file
    const originalName = uploadedFile.originalname
    const tempPath = uploadedFile.path
    const fileExtension = path.extname(originalName).toLowerCase()
    
    console.log('Uploaded file:', {
      originalName,
      tempPath,
      size: uploadedFile.size,
      extension: fileExtension
    })
    
    // é©—è­‰æª”æ¡ˆé¡å‹
    const allowedExtensions = ['.db', '.sql', '.sqlite', '.sqlite3']
    if (!allowedExtensions.includes(fileExtension)) {
      // æ¸…ç†ä¸Šå‚³çš„æª”æ¡ˆ
      fs.unlinkSync(tempPath)
      res.status(400).json({
        success: false,
        message: 'ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œåªå…è¨± .db, .sql, .sqlite, .sqlite3 æª”æ¡ˆ'
      })
      return
    }
    
    // æª¢æŸ¥æª”æ¡ˆå¤§å°ï¼ˆ100MB é™åˆ¶ï¼‰
    if (uploadedFile.size > 100 * 1024 * 1024) {
      fs.unlinkSync(tempPath)
      res.status(400).json({
        success: false,
        message: 'æª”æ¡ˆéå¤§ï¼Œæœ€å¤§æ”¯æ´ 100MB'
      })
      return
    }
    
    const importResult = await importDatabase(tempPath, originalName, (req as any).user.username)

    // æ¸…ç†ä¸Šå‚³çš„è‡¨æ™‚æª”æ¡ˆ
    if (fs.existsSync(tempPath)) {
      fs.unlinkSync(tempPath)
    }

    // è¨˜éŒ„å¯©è¨ˆæ—¥èªŒï¼ˆå¤±æ•—ä¸å½±éŸ¿åŒ¯å…¥çµæœï¼‰
    try {
      const importUser = await db.getUserByUsername((req as any).user.username)
      await db.createAuditLog({
        user_id: importUser?.id || 0,
        username: (req as any).user.username,
        action: 'create',
        resource_type: 'system',
        resource_id: 'database_import',
        details: `è³‡æ–™åº«åŒ¯å…¥æˆåŠŸ: ${originalName}, å¤§å°: ${uploadedFile.size} bytes`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
    } catch (auditError) {
      console.error('Failed to create audit log after import (non-fatal):', auditError)
      // å¯©è¨ˆæ—¥èªŒå¤±æ•—ä¸å½±éŸ¿åŒ¯å…¥æˆåŠŸçš„åˆ¤å®š
    }

    console.log('=== Database Import Completed ===')
    res.json({
      success: true,
      message: 'è³‡æ–™åº«åŒ¯å…¥æˆåŠŸ',
      import: importResult
    })
  } catch (error) {
    console.error('Database import error:', error)
    
    // æ¸…ç†ä¸Šå‚³çš„æª”æ¡ˆ
    if (req.file && fs.existsSync(req.file.path)) {
      fs.unlinkSync(req.file.path)
    }
    
    // è¨˜éŒ„éŒ¯èª¤å¯©è¨ˆæ—¥èªŒ
    try {
      const errorUser = await db.getUserByUsername((req as any).user.username)
      await db.createAuditLog({
        user_id: errorUser?.id || 0,
        username: (req as any).user.username,
        action: 'create',
        resource_type: 'system',
        resource_id: 'database_import',
        details: `è³‡æ–™åº«åŒ¯å…¥å¤±æ•—: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
    } catch (auditError) {
      console.error('Failed to log import error:', auditError)
    }
    
    res.status(500).json({ 
      success: false,
      message: 'è³‡æ–™åº«åŒ¯å…¥å¤±æ•—',
      error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'
    })
  }
})

// å‰µå»ºè³‡æ–™åº«å‚™ä»½çš„æ ¸å¿ƒåŠŸèƒ½
async function createDatabaseBackup(req: any): Promise<{ filename: string; size: number; path: string }> {
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
  const backupDir = path.join(__dirname, '../../backups')
  
  // ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
  if (!fs.existsSync(backupDir)) {
    fs.mkdirSync(backupDir, { recursive: true })
  }
  
  if (process.env.USE_PG === 'true') {
    // PostgreSQL å‚™ä»½
    return await createPostgreSQLBackup(backupDir, timestamp)
  } else {
    // SQLite å‚™ä»½
    return await createSQLiteBackup(backupDir, timestamp)
  }
}

// SQLite å‚™ä»½
async function createSQLiteBackup(backupDir: string, timestamp: string): Promise<{ filename: string; size: number; path: string }> {
  const sourceDb = getDatabasePath()
  const backupFilename = `backup-sqlite-${timestamp}.db`
  const backupPath = path.join(backupDir, backupFilename)
  
  console.log('ğŸ” Source database path:', sourceDb)
  console.log('ğŸ” Backup path:', backupPath)
  
  // æª¢æŸ¥ä¾†æºè³‡æ–™åº«æ˜¯å¦å­˜åœ¨
  if (!fs.existsSync(sourceDb)) {
    console.log('âŒ Database file not found. Available files in directory:')
    try {
      const dirContents = fs.readdirSync(path.dirname(sourceDb))
      console.log('Directory contents:', dirContents)
    } catch (e) {
      console.log('Cannot read directory:', e)
    }
    throw new Error(`SQLite è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨: ${sourceDb}`)
  }
  
  // è¤‡è£½è³‡æ–™åº«æª”æ¡ˆ
  fs.copyFileSync(sourceDb, backupPath)
  
  const stats = fs.statSync(backupPath)
  
  return {
    filename: backupFilename,
    size: stats.size,
    path: backupPath
  }
}

// PostgreSQL å‚™ä»½
async function createPostgreSQLBackup(backupDir: string, timestamp: string): Promise<{ filename: string; size: number; path: string }> {
  const { spawn } = require('child_process')
  const backupFilename = `backup-postgresql-${timestamp}.sql`
  const backupPath = path.join(backupDir, backupFilename)
  
  const pgDumpArgs = [
    '-h', process.env.PG_HOST || 'localhost',
    '-p', process.env.PG_PORT || '5432',
    '-U', process.env.PG_USER || 'postgres',
    '-d', process.env.PG_DATABASE || 'stoner_system',
    '-f', backupPath,
    '--verbose',
    '--no-password'
  ]
  
  return new Promise((resolve, reject) => {
    const pgDump = spawn('pg_dump', pgDumpArgs, {
      env: {
        ...process.env,
        PGPASSWORD: process.env.PG_PASSWORD || ''
      }
    })
    
    pgDump.on('close', (code) => {
      if (code === 0) {
        const stats = fs.statSync(backupPath)
        resolve({
          filename: backupFilename,
          size: stats.size,
          path: backupPath
        })
      } else {
        reject(new Error(`pg_dump åŸ·è¡Œå¤±æ•—ï¼Œé€€å‡ºä»£ç¢¼: ${code}`))
      }
    })
    
    pgDump.on('error', (error) => {
      reject(new Error(`pg_dump åŸ·è¡ŒéŒ¯èª¤: ${error.message}`))
    })
  })
}

// åŒ¯å…¥è³‡æ–™åº«çš„æ ¸å¿ƒåŠŸèƒ½
async function importDatabase(tempPath: string, originalName: string, username: string): Promise<{ filename: string; size: number; originalName: string }> {
  console.log('Starting database import process...')
  
  if (process.env.USE_PG === 'true') {
    // PostgreSQL åŒ¯å…¥
    return await importPostgreSQLDatabase(tempPath, originalName, username)
  } else {
    // SQLite åŒ¯å…¥
    return await importSQLiteDatabase(tempPath, originalName, username)
  }
}

// SQLite è³‡æ–™åº«åŒ¯å…¥
async function importSQLiteDatabase(tempPath: string, originalName: string, username: string): Promise<{ filename: string; size: number; originalName: string }> {
  const currentDbPath = getDatabasePath()
  const backupDir = path.join(__dirname, '../../backups')
  
  console.log('ğŸ” Current database path:', currentDbPath)
  console.log('ğŸ” Temp file path:', tempPath)
  console.log('ğŸ” Backup directory:', backupDir)
  
  // ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
  if (!fs.existsSync(backupDir)) {
    fs.mkdirSync(backupDir, { recursive: true })
  }
  
  // åœ¨åŒ¯å…¥å‰å…ˆå‚™ä»½ç¾æœ‰è³‡æ–™åº«
  if (fs.existsSync(currentDbPath)) {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
    const backupFilename = `backup-before-import-${timestamp}.db`
    const backupPath = path.join(backupDir, backupFilename)
    
    console.log(`Creating backup before import: ${backupFilename}`)
    fs.copyFileSync(currentDbPath, backupPath)
  }
  
  // æª¢æŸ¥ä¸Šå‚³çš„æª”æ¡ˆæ˜¯å¦æœ‰æ•ˆçš„ SQLite è³‡æ–™åº«
  try {
    const fileBuffer = fs.readFileSync(tempPath)
    const fileHeader = fileBuffer.toString('ascii', 0, 16)
    
    if (!fileHeader.startsWith('SQLite format 3')) {
      throw new Error('ä¸Šå‚³çš„æª”æ¡ˆä¸æ˜¯æœ‰æ•ˆçš„ SQLite è³‡æ–™åº«æª”æ¡ˆ')
    }
  } catch (error) {
    throw new Error('ç„¡æ³•è®€å–æˆ–é©—è­‰ä¸Šå‚³çš„è³‡æ–™åº«æª”æ¡ˆ')
  }
  
  // æ›¿æ›ç¾æœ‰è³‡æ–™åº«
  console.log('ğŸ”„ Replacing current database with uploaded file...')
  console.log('ğŸ”„ Before replacement - Current DB size:', fs.existsSync(currentDbPath) ? fs.statSync(currentDbPath).size : 0)
  console.log('ğŸ”„ Uploaded file size:', fs.statSync(tempPath).size)
  
  fs.copyFileSync(tempPath, currentDbPath)
  
  const stats = fs.statSync(currentDbPath)
  
  console.log(`âœ… SQLite database import completed. New database size: ${stats.size} bytes`)
  
  // é©—è­‰åŒ¯å…¥çµæœ
  try {
    const Database = require('better-sqlite3')
    const testDb = new Database(currentDbPath, { readonly: true })
    const tables = testDb.prepare("SELECT name FROM sqlite_master WHERE type='table'").all()
    console.log('âœ… Imported database tables:', tables.map(t => t.name))
    
    // æª¢æŸ¥ä¸»è¦è¡¨çš„è¨˜éŒ„æ•¸
    const paymentCount = testDb.prepare("SELECT COUNT(*) as count FROM payments").get()
    const userCount = testDb.prepare("SELECT COUNT(*) as count FROM users").get()
    const storeCount = testDb.prepare("SELECT COUNT(*) as count FROM stores").get()
    
    console.log('âœ… Record counts after import:')
    console.log('   - Payments:', paymentCount?.count || 0)
    console.log('   - Users:', userCount?.count || 0)
    console.log('   - Stores:', storeCount?.count || 0)
    
    testDb.close()
  } catch (verifyError) {
    console.log('âš ï¸  Could not verify imported database:', verifyError)
  }
  
  return {
    filename: path.basename(currentDbPath),
    size: stats.size,
    originalName: originalName
  }
}

// PostgreSQL è³‡æ–™åº«åŒ¯å…¥
async function importPostgreSQLDatabase(tempPath: string, originalName: string, username: string): Promise<{ filename: string; size: number; originalName: string }> {
  const { spawn } = require('child_process')
  
  // æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚º SQL æª”æ¡ˆ
  if (!originalName.toLowerCase().endsWith('.sql')) {
    throw new Error('PostgreSQL åŒ¯å…¥åªæ”¯æ´ .sql æª”æ¡ˆ')
  }
  
  const psqlArgs = [
    '-h', process.env.PG_HOST || 'localhost',
    '-p', process.env.PG_PORT || '5432',
    '-U', process.env.PG_USER || 'postgres',
    '-d', process.env.PG_DATABASE || 'stoner_system',
    '-f', tempPath,
    '--quiet'
  ]
  
  return new Promise((resolve, reject) => {
    console.log('Starting PostgreSQL import...')
    
    const psql = spawn('psql', psqlArgs, {
      env: {
        ...process.env,
        PGPASSWORD: process.env.PG_PASSWORD || ''
      }
    })
    
    let errorOutput = ''
    
    psql.stderr.on('data', (data) => {
      errorOutput += data.toString()
    })
    
    psql.on('close', (code) => {
      if (code === 0) {
        const stats = fs.statSync(tempPath)
        console.log(`PostgreSQL database import completed. SQL file size: ${stats.size} bytes`)
        
        resolve({
          filename: originalName,
          size: stats.size,
          originalName: originalName
        })
      } else {
        reject(new Error(`psql åŸ·è¡Œå¤±æ•—ï¼Œé€€å‡ºä»£ç¢¼: ${code}ï¼ŒéŒ¯èª¤è¨Šæ¯: ${errorOutput}`))
      }
    })
    
    psql.on('error', (error) => {
      reject(new Error(`psql åŸ·è¡ŒéŒ¯èª¤: ${error.message}`))
    })
  })
}

export { router as backupRoutes }