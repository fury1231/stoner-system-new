import { Router, Request, Response, NextFunction } from 'express'
import { db } from '../db.js'
import { authenticate, AuthenticatedRequest } from '../middleware/auth.js'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'
import multer from 'multer'
import Database from 'better-sqlite3'
import type { PoolClient } from 'pg'

const router: Router = Router()
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

// ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šé©—è­‰ PostgreSQL åƒæ•¸ï¼Œé˜²æ­¢å‘½ä»¤æ³¨å…¥
function sanitizePgParam(param: string, paramName: string): string {
  // åªå…è¨±å­—æ¯ã€æ•¸å­—ã€é»ã€é€£å­—ç¬¦ã€åº•ç·š
  if (!/^[a-zA-Z0-9._-]+$/.test(param)) {
    throw new Error(`PostgreSQL åƒæ•¸ ${paramName} æ ¼å¼ç„¡æ•ˆ`)
  }
  // é™åˆ¶é•·åº¦
  if (param.length > 128) {
    throw new Error(`PostgreSQL åƒæ•¸ ${paramName} éé•·`)
  }
  return param
}

// ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šé©—è­‰ PostgreSQL ç«¯å£
function sanitizePgPort(port: string): string {
  const portNum = parseInt(port, 10)
  if (isNaN(portNum) || portNum < 1 || portNum > 65535) {
    throw new Error('PostgreSQL ç«¯å£å¿…é ˆæ˜¯ 1-65535 ä¹‹é–“çš„æ•¸å­—')
  }
  return port
}

// è¨­ç½® multer ç”¨æ–¼æª”æ¡ˆä¸Šå‚³
const upload = multer({
  dest: path.join(__dirname, '../../uploads/'),
  limits: {
    fileSize: 100 * 1024 * 1024, // 100MB é™åˆ¶
  },
  fileFilter: (req, file, cb) => {
    // å…è¨± PostgreSQL SQL å‚™ä»½æª”æ¡ˆå’Œ SQLite å‚™ä»½æª”æ¡ˆ
    const allowedExtensions = ['.sql', '.db', '.sqlite', '.sqlite3']
    const fileExtension = path.extname(file.originalname).toLowerCase()

    if (allowedExtensions.includes(fileExtension)) {
      cb(null, true)
    } else {
      cb(new Error('åªå…è¨±ä¸Šå‚³ .sql (PostgreSQL) æˆ– .db/.sqlite/.sqlite3 (SQLite) æ ¼å¼çš„å‚™ä»½æª”æ¡ˆ'))
    }
  }
})

// æ¬Šé™æª¢æŸ¥ä¸­é–“ä»¶ - åªæœ‰ç®¡ç†å“¡èƒ½å‚™ä»½
const requireAdmin = async (req: AuthenticatedRequest, res: Response, next: NextFunction) => {
  try {
    const user = await db.getUserByUsername(req.user.username)
    if (!user || user.role !== 'admin') {
      return res.status(403).json({ message: 'æ¬Šé™ä¸è¶³ï¼šåƒ…ç®¡ç†å“¡å¯åŸ·è¡Œå‚™ä»½æ“ä½œ' })
    }
    next()
  } catch (error) {
    console.error('Permission check error:', error)
    res.status(500).json({ message: 'ä¼ºæœå™¨éŒ¯èª¤' })
  }
}

// å‚™ä»½è³‡æ–™åº«
router.post('/database', authenticate, requireAdmin, async (req: AuthenticatedRequest, res: Response): Promise<void> => {
  try {
    console.log('=== Database Backup Started ===')
    const backupResult = await createDatabaseBackup()
    
    // è¨˜éŒ„å¯©è¨ˆæ—¥èªŒ
    const backupUser = await db.getUserByUsername(req.user.username)
    await db.createAuditLog({
      user_id: backupUser?.id || 0,
      username: req.user.username,
      action: 'create',
      resource_type: 'system',
      resource_id: 'database_backup',
      details: `è³‡æ–™åº«å‚™ä»½æˆåŠŸ: ${backupResult.filename}, å¤§å°: ${backupResult.size} bytes`,
      ip_address: req.ip,
      user_agent: req.get('User-Agent')
    })
    
    console.log('=== Database Backup Completed ===')
    res.json({
      success: true,
      message: 'è³‡æ–™åº«å‚™ä»½æˆåŠŸ',
      backup: backupResult
    })
  } catch (error) {
    console.error('Database backup error:', error)
    
    // è¨˜éŒ„éŒ¯èª¤å¯©è¨ˆæ—¥èªŒ
    try {
      const errorUser = await db.getUserByUsername(req.user.username)
      await db.createAuditLog({
        user_id: errorUser?.id || 0,
        username: req.user.username,
        action: 'create',
        resource_type: 'system',
        resource_id: 'database_backup',
        details: `è³‡æ–™åº«å‚™ä»½å¤±æ•—: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
    } catch (auditError) {
      console.error('Failed to log backup error:', auditError)
    }
    
    res.status(500).json({ 
      success: false,
      message: 'è³‡æ–™åº«å‚™ä»½å¤±æ•—',
      error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'
    })
  }
})

// ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šä¸‹è¼‰å‚™ä»½æª”æ¡ˆä½¿ç”¨ Cookie èªè­‰ï¼ˆç§»é™¤ä¸å®‰å…¨çš„ URL tokenï¼‰
// JWT åœ¨ URL ä¸­æœƒæš´éœ²åœ¨ç€è¦½å™¨æ­·å²ã€ä¼ºæœå™¨æ—¥èªŒã€Referrer æ¨™é ­ä¸­

// ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šå‚™ä»½æª”åç™½åå–®é©—è­‰ï¼ˆé˜²æ­¢è·¯å¾‘éæ­·å’Œç·¨ç¢¼ç¹éï¼‰
const SAFE_BACKUP_FILENAME_REGEX = /^backup-(postgresql|sqlite)-\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}-\d{3}Z\.(sql|db)$/

router.get('/download/:filename', authenticate, requireAdmin, async (req: AuthenticatedRequest, res: Response): Promise<void> => {
  try {
    const filename = req.params.filename

    // ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šæª”åç™½åå–®é©—è­‰ï¼ˆåœ¨è·¯å¾‘è™•ç†å‰æª¢æŸ¥ï¼‰
    if (!SAFE_BACKUP_FILENAME_REGEX.test(filename)) {
      res.status(400).json({ message: 'ç„¡æ•ˆçš„æª”æ¡ˆåç¨±æ ¼å¼' })
      return
    }

    // ç²å–ç”¨æˆ¶ä¿¡æ¯ï¼ˆå·²é€šé authenticate å’Œ requireAdmin ä¸­é–“ä»¶é©—è­‰ï¼‰
    const user = await db.getUserByUsername(req.user.username)
    if (!user) {
      res.status(401).json({ message: 'ç”¨æˆ¶ä¸å­˜åœ¨' })
      return
    }

    const backupDir = path.join(__dirname, '../../backups')
    const filePath = path.join(backupDir, filename)

    // å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æª”æ¡ˆè·¯å¾‘åœ¨å‚™ä»½ç›®éŒ„å…§ï¼ˆé›™é‡ä¿è­·ï¼‰
    const normalizedBackupDir = path.resolve(backupDir)
    const normalizedFilePath = path.resolve(filePath)

    if (!normalizedFilePath.startsWith(normalizedBackupDir)) {
      res.status(400).json({ message: 'ç„¡æ•ˆçš„æª”æ¡ˆè·¯å¾‘' })
      return
    }

    // æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if (!fs.existsSync(filePath)) {
      res.status(404).json({ message: 'å‚™ä»½æª”æ¡ˆä¸å­˜åœ¨' })
      return
    }
    
    // è¨˜éŒ„ä¸‹è¼‰å¯©è¨ˆæ—¥èªŒï¼ˆå¤±æ•—ä¸å½±éŸ¿ä¸‹è¼‰ï¼‰
    try {
      await db.createAuditLog({
        user_id: user.id,
        username: req.user.username,
        action: 'view',
        resource_type: 'system',
        resource_id: 'database_backup',
        details: `ä¸‹è¼‰å‚™ä»½æª”æ¡ˆ: ${filename}`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
    } catch {
      // å¯©è¨ˆæ—¥èªŒå¤±æ•—ä¸å½±éŸ¿ä¸‹è¼‰
    }

    // è¨­å®šä¸‹è¼‰æ¨™é ­
    res.setHeader('Content-Disposition', `attachment; filename="${filename}"`)
    res.setHeader('Content-Type', 'application/octet-stream')

    // å‚³é€æª”æ¡ˆ
    res.sendFile(filePath, (err) => {
      if (err && !res.headersSent) {
        res.status(500).json({ message: 'ä¸‹è¼‰å‚™ä»½æª”æ¡ˆå¤±æ•—' })
      }
    })
  } catch (error) {
    console.error('Download backup error:', error)
    res.status(500).json({ message: 'ä¸‹è¼‰å‚™ä»½æª”æ¡ˆå¤±æ•—' })
  }
})

// åˆ—å‡ºå¯ç”¨çš„å‚™ä»½æª”æ¡ˆ
router.get('/list', authenticate, requireAdmin, async (req: AuthenticatedRequest, res: Response): Promise<void> => {
  try {
    const backupDir = path.join(__dirname, '../../backups')
    
    // ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
    if (!fs.existsSync(backupDir)) {
      fs.mkdirSync(backupDir, { recursive: true })
    }
    
    const files = fs.readdirSync(backupDir)
      .filter(file => file.endsWith('.sql') || file.endsWith('.db'))
      .map(file => {
        const filePath = path.join(backupDir, file)
        const stats = fs.statSync(filePath)
        return {
          filename: file,
          size: stats.size,
          created: stats.birthtime,
          modified: stats.mtime
        }
      })
      .sort((a, b) => b.created.getTime() - a.created.getTime())
    
    res.json({
      success: true,
      backups: files
    })
  } catch (error) {
    console.error('List backups error:', error)
    res.status(500).json({ message: 'ç²å–å‚™ä»½åˆ—è¡¨å¤±æ•—' })
  }
})

// åŒ¯å…¥è³‡æ–™åº«
router.post('/import', authenticate, requireAdmin, upload.single('database'), async (req: AuthenticatedRequest, res: Response): Promise<void> => {
  try {
    console.log('=== Database Import Started ===')
    
    if (!req.file) {
      res.status(400).json({ 
        success: false, 
        message: 'æœªæä¾›è³‡æ–™åº«æª”æ¡ˆ' 
      })
      return
    }
    
    const uploadedFile = req.file
    const originalName = uploadedFile.originalname
    const tempPath = uploadedFile.path
    const fileExtension = path.extname(originalName).toLowerCase()
    
    console.log('Uploaded file:', {
      originalName,
      tempPath,
      size: uploadedFile.size,
      extension: fileExtension
    })
    
    // å…è¨± PostgreSQL SQL å‚™ä»½æª”æ¡ˆå’Œ SQLite å‚™ä»½æª”æ¡ˆ
    const allowedExtensions = ['.sql', '.db', '.sqlite', '.sqlite3']
    if (!allowedExtensions.includes(fileExtension)) {
      // æ¸…ç†ä¸Šå‚³çš„æª”æ¡ˆ
      fs.unlinkSync(tempPath)
      res.status(400).json({
        success: false,
        message: 'ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œåªå…è¨± .sql (PostgreSQL) æˆ– .db/.sqlite/.sqlite3 (SQLite) æ ¼å¼'
      })
      return
    }
    
    // æª¢æŸ¥æª”æ¡ˆå¤§å°ï¼ˆ100MB é™åˆ¶ï¼‰
    if (uploadedFile.size > 100 * 1024 * 1024) {
      fs.unlinkSync(tempPath)
      res.status(400).json({
        success: false,
        message: 'æª”æ¡ˆéå¤§ï¼Œæœ€å¤§æ”¯æ´ 100MB'
      })
      return
    }
    
    const importResult = await importDatabase(tempPath, originalName, req.user.username)

    // æ¸…ç†ä¸Šå‚³çš„è‡¨æ™‚æª”æ¡ˆ
    if (fs.existsSync(tempPath)) {
      fs.unlinkSync(tempPath)
    }

    // è¨˜éŒ„å¯©è¨ˆæ—¥èªŒï¼ˆå¤±æ•—ä¸å½±éŸ¿åŒ¯å…¥çµæœï¼‰
    try {
      const importUser = await db.getUserByUsername(req.user.username)
      await db.createAuditLog({
        user_id: importUser?.id || 0,
        username: req.user.username,
        action: 'create',
        resource_type: 'system',
        resource_id: 'database_import',
        details: `è³‡æ–™åº«åŒ¯å…¥æˆåŠŸ: ${originalName}, å¤§å°: ${uploadedFile.size} bytes`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
    } catch (auditError) {
      console.error('Failed to create audit log after import (non-fatal):', auditError)
      // å¯©è¨ˆæ—¥èªŒå¤±æ•—ä¸å½±éŸ¿åŒ¯å…¥æˆåŠŸçš„åˆ¤å®š
    }

    console.log('=== Database Import Completed ===')
    res.json({
      success: true,
      message: 'è³‡æ–™åº«åŒ¯å…¥æˆåŠŸ',
      import: importResult
    })
  } catch (error) {
    console.error('Database import error:', error)
    
    // æ¸…ç†ä¸Šå‚³çš„æª”æ¡ˆ
    if (req.file && fs.existsSync(req.file.path)) {
      fs.unlinkSync(req.file.path)
    }
    
    // è¨˜éŒ„éŒ¯èª¤å¯©è¨ˆæ—¥èªŒ
    try {
      const errorUser = await db.getUserByUsername(req.user.username)
      await db.createAuditLog({
        user_id: errorUser?.id || 0,
        username: req.user.username,
        action: 'create',
        resource_type: 'system',
        resource_id: 'database_import',
        details: `è³‡æ–™åº«åŒ¯å…¥å¤±æ•—: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`,
        ip_address: req.ip,
        user_agent: req.get('User-Agent')
      })
    } catch (auditError) {
      console.error('Failed to log import error:', auditError)
    }
    
    res.status(500).json({ 
      success: false,
      message: 'è³‡æ–™åº«åŒ¯å…¥å¤±æ•—',
      error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'
    })
  }
})

// å‰µå»ºè³‡æ–™åº«å‚™ä»½çš„æ ¸å¿ƒåŠŸèƒ½
async function createDatabaseBackup(): Promise<{ filename: string; size: number; path: string }> {
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
  const backupDir = path.join(__dirname, '../../backups')

  // ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
  if (!fs.existsSync(backupDir)) {
    fs.mkdirSync(backupDir, { recursive: true })
  }

  // ç¾åœ¨åªä½¿ç”¨ PostgreSQL
  return await createPostgreSQLBackup(backupDir, timestamp)
}

// PostgreSQL å‚™ä»½
async function createPostgreSQLBackup(backupDir: string, timestamp: string): Promise<{ filename: string; size: number; path: string }> {
  const { spawn } = require('child_process')
  const backupFilename = `backup-postgresql-${timestamp}.sql`
  const backupPath = path.join(backupDir, backupFilename)

  // ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šé©—è­‰æ‰€æœ‰ PostgreSQL åƒæ•¸
  const pgHost = sanitizePgParam(process.env.PG_HOST || 'localhost', 'PG_HOST')
  const pgPort = sanitizePgPort(process.env.PG_PORT || '5432')
  const pgUsername = sanitizePgParam(process.env.PG_USERNAME || 'stoner', 'PG_USERNAME')
  const pgDatabase = sanitizePgParam(process.env.PG_DATABASE || 'stoner_system', 'PG_DATABASE')

  const pgDumpArgs = [
    '-h', pgHost,
    '-p', pgPort,
    '-U', pgUsername,
    '-d', pgDatabase,
    '-f', backupPath,
    '--verbose',
    '--no-password'
  ]

  return new Promise((resolve, reject) => {
    const pgDump = spawn('pg_dump', pgDumpArgs, {
      env: {
        ...process.env,
        PGPASSWORD: process.env.PG_PASSWORD || ''
      }
    })

    // æ•ç² stderr ä»¥ç²å–éŒ¯èª¤è¨Šæ¯
    let stderr = ''
    pgDump.stderr.on('data', (data: Buffer) => {
      stderr += data.toString()
    })

    pgDump.on('close', (code: number | null) => {
      if (code === 0) {
        const stats = fs.statSync(backupPath)
        resolve({
          filename: backupFilename,
          size: stats.size,
          path: backupPath
        })
      } else {
        // æ¸…ç†ä¸å®Œæ•´çš„å‚™ä»½æª”æ¡ˆ
        if (fs.existsSync(backupPath)) {
          try {
            fs.unlinkSync(backupPath)
          } catch (cleanupError) {
            console.error('Failed to clean up partial backup file:', cleanupError)
          }
        }
        reject(new Error(`pg_dump åŸ·è¡Œå¤±æ•—: ${stderr || `é€€å‡ºä»£ç¢¼ ${code}`}`))
      }
    })

    pgDump.on('error', (error: Error) => {
      // æ¸…ç†ä¸å®Œæ•´çš„å‚™ä»½æª”æ¡ˆ
      if (fs.existsSync(backupPath)) {
        try {
          fs.unlinkSync(backupPath)
        } catch (cleanupError) {
          console.error('Failed to clean up partial backup file:', cleanupError)
        }
      }
      reject(new Error(`pg_dump åŸ·è¡ŒéŒ¯èª¤: ${error.message}`))
    })
  })
}

// åŒ¯å…¥è³‡æ–™åº«çš„æ ¸å¿ƒåŠŸèƒ½
async function importDatabase(tempPath: string, originalName: string, _username: string): Promise<{ filename: string; size: number; originalName: string }> {
  const fileExtension = path.extname(originalName).toLowerCase()

  if (fileExtension === '.sql') {
    // PostgreSQL SQL å‚™ä»½æª”æ¡ˆé‚„åŸ
    return await importPostgreSQLBackup(tempPath, originalName)
  }

  if (['.db', '.sqlite', '.sqlite3'].includes(fileExtension)) {
    // SQLite å‚™ä»½æª”æ¡ˆè½‰æ›ä¸¦åŒ¯å…¥ PostgreSQL
    return await importSQLiteToPostgreSQL(tempPath, originalName)
  }

  throw new Error(`ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: ${fileExtension}ï¼Œåªæ”¯æ´ .sql æˆ– .db/.sqlite/.sqlite3 æ ¼å¼`)
}

// PostgreSQL å‚™ä»½é‚„åŸ
async function importPostgreSQLBackup(tempPath: string, originalName: string): Promise<{ filename: string; size: number; originalName: string }> {
  const { spawn } = require('child_process')
  const fileStats = fs.statSync(tempPath)

  // æª¢æŸ¥æª”æ¡ˆå¤§å°
  const maxFileSize = 100 * 1024 * 1024 // 100MB
  if (fileStats.size > maxFileSize) {
    throw new Error('å‚™ä»½æª”æ¡ˆéå¤§ï¼Œæœ€å¤§å…è¨± 100MB')
  }
  if (fileStats.size < 100) {
    throw new Error('å‚™ä»½æª”æ¡ˆéå°ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ PostgreSQL å‚™ä»½')
  }

  // é©—è­‰æª”æ¡ˆå…§å®¹ï¼ˆæª¢æŸ¥æ˜¯å¦ç‚º pg_dump ç”¢ç”Ÿçš„ SQLï¼‰
  const fileContent = fs.readFileSync(tempPath, 'utf8').substring(0, 1000)
  if (!fileContent.includes('PostgreSQL database dump') && !fileContent.includes('pg_dump')) {
    throw new Error('ä¸Šå‚³çš„æª”æ¡ˆä¸æ˜¯æœ‰æ•ˆçš„ PostgreSQL å‚™ä»½æª”æ¡ˆ')
  }

  // ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šé©—è­‰æ‰€æœ‰ PostgreSQL åƒæ•¸
  const pgHost = sanitizePgParam(process.env.PG_HOST || 'localhost', 'PG_HOST')
  const pgPort = sanitizePgPort(process.env.PG_PORT || '5432')
  const pgUsername = sanitizePgParam(process.env.PG_USERNAME || 'stoner', 'PG_USERNAME')
  const pgDatabase = sanitizePgParam(process.env.PG_DATABASE || 'stoner_system', 'PG_DATABASE')

  const psqlArgs = [
    '-h', pgHost,
    '-p', pgPort,
    '-U', pgUsername,
    '-d', pgDatabase,
    '-f', tempPath,
    '--quiet'
  ]

  const IMPORT_TIMEOUT_MS = 120000 // 2 åˆ†é˜è¶…æ™‚ (CLAUDE.md v3.22)

  return new Promise((resolve, reject) => {
    let isTimedOut = false

    const psql = spawn('psql', psqlArgs, {
      env: {
        ...process.env,
        PGPASSWORD: process.env.PG_PASSWORD || ''
      }
    })

    // è¨­ç½®è¶…æ™‚è¨ˆæ™‚å™¨
    const timeoutId = setTimeout(() => {
      isTimedOut = true
      psql.kill('SIGTERM')
      reject(new Error('psql åŸ·è¡Œè¶…æ™‚ï¼ˆè¶…é 2 åˆ†é˜ï¼‰'))
    }, IMPORT_TIMEOUT_MS)

    let stderr = ''
    psql.stderr.on('data', (data: Buffer) => {
      stderr += data.toString()
    })

    psql.on('close', (code: number | null) => {
      clearTimeout(timeoutId)
      if (isTimedOut) return // å·²ç¶“å› è¶…æ™‚è€Œ reject

      if (code === 0) {
        resolve({
          filename: originalName,
          size: fileStats.size,
          originalName: originalName
        })
      } else {
        reject(new Error(`psql åŸ·è¡Œå¤±æ•—: ${stderr || `é€€å‡ºä»£ç¢¼ ${code}`}`))
      }
    })

    psql.on('error', (error: Error) => {
      clearTimeout(timeoutId)
      if (isTimedOut) return
      reject(new Error(`psql åŸ·è¡ŒéŒ¯èª¤: ${error.message}`))
    })
  })
}

// ğŸ”’ SQLite Schema ç™½åå–®é©—è­‰
const EXPECTED_SQLITE_TABLES = ['stores', 'users', 'payments', 'audit_logs', 'customer_orders'] as const
const EXPECTED_COLUMNS: Record<string, string[]> = {
  stores: ['id', 'name', 'code', 'address', 'phone', 'manager', 'is_active', 'created_at'],
  users: ['id', 'username', 'password_hash', 'role', 'permissions', 'store_id', 'accessible_stores', 'is_active', 'created_at', 'updated_at'],
  payments: ['uuid', 'last_five', 'paid_at', 'amount', 'note', 'status', 'store_id', 'payment_method', 'processed_by', 'created_at'],
  audit_logs: ['id', 'user_id', 'username', 'action', 'resource_type', 'resource_id', 'details', 'ip_address', 'user_agent', 'created_at'],
  customer_orders: ['id', 'order_date', 'products', 'customer_name', 'customer_phone', 'payment_status', 'logistics', 'remarks', 'amount', 'status', 'store_id', 'created_by', 'created_at', 'updated_at']
}

// ğŸ”’ å®‰å…¨æ€§æ”¹é€²ï¼šä½¿ç”¨ Set é€²è¡Œ O(1) ç™½åå–®æŸ¥è©¢
const EXPECTED_SQLITE_TABLES_SET = new Set(['stores', 'users', 'payments', 'audit_logs', 'customer_orders'])

// SQLite å‚™ä»½æª”æ¡ˆè½‰æ›ä¸¦åŒ¯å…¥ PostgreSQL
async function importSQLiteToPostgreSQL(tempPath: string, originalName: string): Promise<{ filename: string; size: number; originalName: string }> {
  const fileStats = fs.statSync(tempPath)

  // æª¢æŸ¥æª”æ¡ˆå¤§å°
  const maxFileSize = 100 * 1024 * 1024 // 100MB
  if (fileStats.size > maxFileSize) {
    throw new Error('å‚™ä»½æª”æ¡ˆéå¤§ï¼Œæœ€å¤§å…è¨± 100MB')
  }
  if (fileStats.size < 100) {
    throw new Error('å‚™ä»½æª”æ¡ˆéå°ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ SQLite å‚™ä»½')
  }

  // ğŸ”’ å®‰å…¨æ€§æª¢æŸ¥ï¼šé©—è­‰ SQLite æª”æ¡ˆæ ¼å¼ï¼ˆSQLite æª”æ¡ˆä»¥ "SQLite format 3" é–‹é ­ï¼‰
  const fileHeader = Buffer.alloc(16)
  const fd = fs.openSync(tempPath, 'r')
  fs.readSync(fd, fileHeader, 0, 16, 0)
  fs.closeSync(fd)

  if (!fileHeader.toString('utf8', 0, 15).startsWith('SQLite format 3')) {
    throw new Error('ä¸Šå‚³çš„æª”æ¡ˆä¸æ˜¯æœ‰æ•ˆçš„ SQLite è³‡æ–™åº«æª”æ¡ˆ')
  }

  // é–‹å•Ÿ SQLite è³‡æ–™åº«ï¼ˆå”¯è®€æ¨¡å¼ï¼‰
  let sqliteDb: Database.Database
  try {
    sqliteDb = new Database(tempPath, { readonly: true, fileMustExist: true })
  } catch (error) {
    throw new Error(`ç„¡æ³•é–‹å•Ÿ SQLite è³‡æ–™åº«: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`)
  }

  // å–å¾— PostgreSQL é€£ç·šæ± å’Œå®¢æˆ¶ç«¯
  const pgPool = db.getPool()
  const client = await pgPool.connect()

  try {
    // ğŸ”’ å®‰å…¨æ€§æª¢æŸ¥ï¼šé©—è­‰ Schema ç™½åå–®
    const tables = sqliteDb.prepare("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'").all() as { name: string }[]
    const tableNames = tables.map(t => t.name)

    // æª¢æŸ¥æ˜¯å¦æœ‰æœªçŸ¥çš„è¡¨ï¼ˆä½¿ç”¨ Set é€²è¡Œå®‰å…¨çš„ç™½åå–®é©—è­‰ï¼‰
    for (const tableName of tableNames) {
      if (!EXPECTED_SQLITE_TABLES_SET.has(tableName)) {
        throw new Error(`SQLite è³‡æ–™åº«åŒ…å«æœªé æœŸçš„è¡¨: ${tableName}`)
      }
    }

    // æª¢æŸ¥æ¯å€‹è¡¨çš„æ¬„ä½ï¼ˆåªæª¢æŸ¥ç™½åå–®å…§çš„è¡¨ï¼‰
    for (const tableName of tableNames) {
      if (!EXPECTED_SQLITE_TABLES_SET.has(tableName)) continue

      // ä½¿ç”¨å®‰å…¨çš„è¡¨åæŸ¥è©¢ï¼ˆå·²é€šéç™½åå–®é©—è­‰ï¼‰
      const columns = sqliteDb.prepare(`PRAGMA table_info(${tableName})`).all() as { name: string }[]
      const columnNames = columns.map(c => c.name)
      const expectedColumns = EXPECTED_COLUMNS[tableName]

      if (expectedColumns) {
        for (const col of columnNames) {
          if (!expectedColumns.includes(col)) {
            throw new Error(`è¡¨ ${tableName} åŒ…å«æœªé æœŸçš„æ¬„ä½: ${col}`)
          }
        }
      }
    }

    // ğŸ”’ ä½¿ç”¨ Transaction ç¢ºä¿è³‡æ–™å®Œæ•´æ€§
    console.log('SQLite schema validation passed, starting import with transaction...')
    await client.query('BEGIN')

    try {
      // åŒ¯å…¥é †åºï¼ˆä¾å¤–éµé—œä¿‚ï¼‰
      const importOrder = ['stores', 'users', 'payments', 'customer_orders', 'audit_logs']
      const BATCH_SIZE = 100 // æ‰¹æ¬¡è™•ç†å¤§å°

      for (const tableName of importOrder) {
        if (!tableNames.includes(tableName)) {
          console.log(`Table ${tableName} not found in SQLite, skipping...`)
          continue
        }

        // ä½¿ç”¨å®‰å…¨çš„è¡¨åæŸ¥è©¢ï¼ˆå·²é€šéç™½åå–®é©—è­‰ï¼‰
        const rows = sqliteDb.prepare(`SELECT * FROM ${tableName}`).all() as Record<string, unknown>[]
        console.log(`Importing ${rows.length} rows from ${tableName}...`)

        if (rows.length === 0) continue

        // æ‰¹æ¬¡è™•ç†åŒ¯å…¥
        for (let i = 0; i < rows.length; i += BATCH_SIZE) {
          const batch = rows.slice(i, i + BATCH_SIZE)
          for (const row of batch) {
            await importRowToPostgreSQLWithClient(client, tableName, row)
          }
          // æ¯æ‰¹æ¬¡å¾ŒçŸ­æš«è®“å‡ºæ§åˆ¶æ¬Šï¼Œé¿å…é˜»å¡
          if (i + BATCH_SIZE < rows.length) {
            await new Promise(resolve => setImmediate(resolve))
          }
        }
      }

      // ğŸ”’ é‡è¨­ PostgreSQL åºåˆ—ï¼Œé¿å…ä¸»éµè¡çª
      console.log('Resetting PostgreSQL sequences...')
      await client.query(`SELECT setval('stores_id_seq', COALESCE((SELECT MAX(id) FROM stores), 0) + 1, false)`)
      await client.query(`SELECT setval('users_id_seq', COALESCE((SELECT MAX(id) FROM users), 0) + 1, false)`)
      await client.query(`SELECT setval('audit_logs_id_seq', COALESCE((SELECT MAX(id) FROM audit_logs), 0) + 1, false)`)
      await client.query(`SELECT setval('customer_orders_id_seq', COALESCE((SELECT MAX(id) FROM customer_orders), 0) + 1, false)`)

      // æäº¤äº¤æ˜“
      await client.query('COMMIT')
      console.log('SQLite to PostgreSQL import completed successfully')

      return {
        filename: originalName,
        size: fileStats.size,
        originalName: originalName
      }
    } catch (importError) {
      // ç™¼ç”ŸéŒ¯èª¤æ™‚å›æ»¾äº¤æ˜“
      await client.query('ROLLBACK')
      console.error('Import failed, transaction rolled back')
      throw importError
    }
  } finally {
    client.release()
    sqliteDb.close()
  }
}

// å°‡å–®è¡Œè³‡æ–™åŒ¯å…¥ PostgreSQLï¼ˆä½¿ç”¨ Client å’Œåƒæ•¸åŒ–æŸ¥è©¢é˜²æ­¢ SQL æ³¨å…¥ï¼‰
async function importRowToPostgreSQLWithClient(client: PoolClient, tableName: string, row: Record<string, unknown>): Promise<void> {
  switch (tableName) {
    case 'stores': {
      const { id, name, code, address, phone, manager, is_active, created_at } = row as any
      await client.query(
        `INSERT INTO stores (id, name, code, address, phone, manager, is_active, created_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
         ON CONFLICT (id) DO UPDATE SET
           name = EXCLUDED.name, code = EXCLUDED.code, address = EXCLUDED.address,
           phone = EXCLUDED.phone, manager = EXCLUDED.manager, is_active = EXCLUDED.is_active`,
        [id, name, code, address, phone, manager, Boolean(is_active), created_at]
      )
      break
    }
    case 'users': {
      const { id, username, password_hash, role, permissions, store_id, accessible_stores, is_active, created_at } = row as any
      await client.query(
        `INSERT INTO users (id, username, password_hash, role, permissions, store_id, accessible_stores, is_active, created_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
         ON CONFLICT (id) DO UPDATE SET
           username = EXCLUDED.username, password_hash = EXCLUDED.password_hash, role = EXCLUDED.role,
           permissions = EXCLUDED.permissions, store_id = EXCLUDED.store_id, accessible_stores = EXCLUDED.accessible_stores,
           is_active = EXCLUDED.is_active`,
        [id, username, password_hash, role, permissions, store_id, accessible_stores, Boolean(is_active), created_at]
      )
      break
    }
    case 'payments': {
      const { uuid, last_five, paid_at, amount, note, status, store_id, payment_method, processed_by, created_at } = row as any
      await client.query(
        `INSERT INTO payments (uuid, last_five, paid_at, amount, note, status, store_id, payment_method, processed_by, created_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
         ON CONFLICT (uuid) DO UPDATE SET
           last_five = EXCLUDED.last_five, paid_at = EXCLUDED.paid_at, amount = EXCLUDED.amount,
           note = EXCLUDED.note, status = EXCLUDED.status, store_id = EXCLUDED.store_id,
           payment_method = EXCLUDED.payment_method, processed_by = EXCLUDED.processed_by`,
        [uuid, last_five, paid_at, amount ?? 0, note, status, store_id, payment_method, processed_by, created_at]
      )
      break
    }
    case 'customer_orders': {
      const { id, order_date, products, customer_name, customer_phone, payment_status, logistics, remarks, amount, status, store_id, created_by, created_at, updated_at } = row as any
      await client.query(
        `INSERT INTO customer_orders (id, order_date, products, customer_name, customer_phone, payment_status, logistics, remarks, amount, status, store_id, created_by, created_at, updated_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
         ON CONFLICT (id) DO UPDATE SET
           order_date = EXCLUDED.order_date, products = EXCLUDED.products, customer_name = EXCLUDED.customer_name,
           customer_phone = EXCLUDED.customer_phone, payment_status = EXCLUDED.payment_status, logistics = EXCLUDED.logistics,
           remarks = EXCLUDED.remarks, amount = EXCLUDED.amount, status = EXCLUDED.status,
           store_id = EXCLUDED.store_id, created_by = EXCLUDED.created_by, updated_at = EXCLUDED.updated_at`,
        [id, order_date, products, customer_name, customer_phone, payment_status, logistics, remarks, amount ?? 0, status, store_id, created_by, created_at, updated_at]
      )
      break
    }
    case 'audit_logs': {
      const { id, user_id, username, action, resource_type, resource_id, details, ip_address, user_agent, created_at } = row as any
      await client.query(
        `INSERT INTO audit_logs (id, user_id, username, action, resource_type, resource_id, details, ip_address, user_agent, created_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
         ON CONFLICT (id) DO NOTHING`,
        [id, user_id, username, action, resource_type, resource_id, details, ip_address, user_agent, created_at]
      )
      break
    }
  }
}

export { router as backupRoutes }